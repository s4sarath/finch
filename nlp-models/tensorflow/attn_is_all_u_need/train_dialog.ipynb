{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook written by [Zhedong Zheng](https://github.com/zhedongzheng)\n",
    "\n",
    "<img src=\"transformer.png\" width=\"200\">\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunch import Bunch\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch({\n",
    "    'source_max_len': 10,\n",
    "    'target_max_len': 10,\n",
    "    'min_freq': 50,\n",
    "    'hidden_units': 128,\n",
    "    'num_blocks': 2,\n",
    "    'num_heads': 8,\n",
    "    'num_heads': 8,\n",
    "    'dropout_rate': 0.1,\n",
    "    'batch_size': 64,\n",
    "    'position_encoding': 'param',\n",
    "    'activation': 'relu',\n",
    "    'tied_proj_weight': True,\n",
    "    'tied_embedding': False,\n",
    "    'label_smoothing': False,\n",
    "    'lr_decay_strategy': 'exp',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, source_path, target_path):\n",
    "        self.source_words = self.read_data(source_path)\n",
    "        self.target_words = self.read_data(target_path)\n",
    "\n",
    "        self.source_word2idx = self.build_index(self.source_words)\n",
    "        self.target_word2idx = self.build_index(self.target_words, is_target=True)\n",
    "\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "\n",
    "    def build_index(self, data, is_target=False):\n",
    "        chars = [char for line in data.split('\\n') for char in line]\n",
    "        chars = [char for char, freq in Counter(chars).items() if freq > args.min_freq]\n",
    "        if is_target:\n",
    "            symbols = ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "        else:\n",
    "            symbols = ['<pad>','<unk>'] if not args.tied_embedding else ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "\n",
    "\n",
    "    def pad(self, data, word2idx, max_len, is_target=False):\n",
    "        res = []\n",
    "        for line in data.split('\\n'):\n",
    "            temp_line = [word2idx.get(char, word2idx['<unk>']) for char in line]\n",
    "            if len(temp_line) >= max_len:\n",
    "                if is_target:\n",
    "                    temp_line = temp_line[:(max_len-1)] + [word2idx['<end>']]\n",
    "                else:\n",
    "                    temp_line = temp_line[:max_len]\n",
    "            if len(temp_line) < max_len:\n",
    "                if is_target:\n",
    "                    temp_line += ([word2idx['<end>']] + [word2idx['<pad>']]*(max_len-len(temp_line)-1)) \n",
    "                else:\n",
    "                    temp_line += [word2idx['<pad>']] * (max_len - len(temp_line))\n",
    "            res.append(temp_line)\n",
    "        return np.array(res)\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        source_idx = self.pad(self.source_words, self.source_word2idx, args.source_max_len)\n",
    "        target_idx = self.pad(self.target_words, self.target_word2idx, args.target_max_len, is_target=True)\n",
    "        return source_idx, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    \n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def embed_seq(inputs, vocab_size, embed_dim, zero_pad=False, scale=False):\n",
    "    lookup_table = tf.get_variable('lookup_table', dtype=tf.float32, shape=[vocab_size, embed_dim])\n",
    "    if zero_pad:\n",
    "        lookup_table = tf.concat((tf.zeros([1, embed_dim]), lookup_table[1:, :]), axis=0)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "    if scale:\n",
    "        outputs = outputs * np.sqrt(embed_dim)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def multihead_attn(queries, keys, q_masks, k_masks, future_binding, is_training):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q]\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k]\n",
    "    \"\"\"\n",
    "    num_units = args.hidden_units\n",
    "    num_heads = args.num_heads\n",
    "    dropout_rate = args.dropout_rate\n",
    "    \n",
    "    T_q = queries.get_shape().as_list()[1]                                         # max time length of query\n",
    "    T_k = keys.get_shape().as_list()[1]                                            # max time length of key\n",
    "\n",
    "    Q = tf.layers.dense(queries, num_units, name='Q')                              # (N, T_q, C)\n",
    "    K_V = tf.layers.dense(keys, 2*num_units, name='K_V')    \n",
    "    K, V = tf.split(K_V, 2, -1)        \n",
    "\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)                         # (h*N, T_q, C/h) \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h) \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h)\n",
    "\n",
    "    # Scaled Dot-Product\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0,2,1]))                               # (h*N, T_q, T_k)\n",
    "    align = align / np.sqrt(K_.get_shape().as_list()[-1])                          # scale\n",
    "\n",
    "    # Key Masking\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))                             # exp(-large) -> 0\n",
    "\n",
    "    key_masks = k_masks                                                            # (N, T_k)\n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1])                                 # (h*N, T_k)\n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, T_q, 1])                 # (h*N, T_q, T_k)\n",
    "    align = tf.where(tf.equal(key_masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "\n",
    "    if future_binding:\n",
    "        lower_tri = tf.ones([T_q, T_k])                                            # (T_q, T_k)\n",
    "        lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0], 1, 1])   # (h*N, T_q, T_k)\n",
    "        align = tf.where(tf.equal(masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "    \n",
    "    # Softmax\n",
    "    align = tf.nn.softmax(align)                                                   # (h*N, T_q, T_k)\n",
    "\n",
    "    # Query Masking\n",
    "    query_masks = tf.to_float(q_masks)                                             # (N, T_q)\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1])                             # (h*N, T_q)\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, T_k])            # (h*N, T_q, T_k)\n",
    "    align *= query_masks                                                           # (h*N, T_q, T_k)\n",
    "\n",
    "    align = tf.layers.dropout(align, dropout_rate, training=is_training)           # (h*N, T_q, T_k)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(align, V_)                                                 # (h*N, T_q, C/h)\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)              # (N, T_q, C)\n",
    "    # Residual connection\n",
    "    outputs += queries                                                             # (N, T_q, C)   \n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)                                                  # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def pointwise_feedforward(inputs, activation=None):\n",
    "    num_units = [4*args.hidden_units, args.hidden_units]\n",
    "    # Inner layer\n",
    "    outputs = tf.layers.conv1d(inputs, num_units[0], kernel_size=1, activation=activation)\n",
    "    # Readout layer\n",
    "    outputs = tf.layers.conv1d(outputs, num_units[1], kernel_size=1, activation=None)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def learned_position_encoding(inputs, mask, embed_dim):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    outputs = tf.range(tf.shape(inputs)[1])                # (T_q)\n",
    "    outputs = tf.expand_dims(outputs, 0)                   # (1, T_q)\n",
    "    outputs = tf.tile(outputs, [tf.shape(inputs)[0], 1])   # (N, T_q)\n",
    "    outputs = embed_seq(outputs, T, embed_dim, zero_pad=False, scale=False)\n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, num_units):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    position_idx = tf.tile(tf.expand_dims(tf.range(T), 0), [tf.shape(inputs)[0], 1])\n",
    "\n",
    "    position_enc = np.array(\n",
    "        [[pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T)])\n",
    "    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    lookup_table = tf.convert_to_tensor(position_enc, tf.float32)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, position_idx)\n",
    "    \n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    C = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1 - epsilon) * inputs) + (epsilon / C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sources, targets, mode, params):\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    pos_enc = _get_position_encoder()\n",
    "\n",
    "    # ENCODER\n",
    "    en_masks = tf.sign(sources)   \n",
    "\n",
    "    with tf.variable_scope('encoder_embedding'):\n",
    "        encoded = embed_seq(inputs = sources,\n",
    "                            vocab_size = params['source_vocab_size'],\n",
    "                            embed_dim = args.hidden_units,\n",
    "                            zero_pad = True,\n",
    "                            scale = True)\n",
    "\n",
    "    with tf.variable_scope('encoder_position_encoding'):\n",
    "        encoded += pos_enc(sources, en_masks, args.hidden_units)\n",
    "\n",
    "    with tf.variable_scope('encoder_dropout'):\n",
    "        encoded = tf.layers.dropout(encoded, args.dropout_rate, training=is_training)\n",
    "\n",
    "    for i in range(args.num_blocks):\n",
    "        with tf.variable_scope('encoder_attn_%d'%i):\n",
    "            encoded = multihead_attn(queries = encoded,\n",
    "                                     keys = encoded,\n",
    "                                     q_masks = en_masks,\n",
    "                                     k_masks = en_masks,\n",
    "                                     future_binding = False,\n",
    "                                     is_training = is_training,)\n",
    "\n",
    "        with tf.variable_scope('encoder_feedforward_%d'%i):\n",
    "            encoded = pointwise_feedforward(encoded,\n",
    "                                            activation = params['activation'])\n",
    "\n",
    "    # DECODER\n",
    "    decoder_inputs = _shift_right(targets, params['start_symbol'])\n",
    "    de_masks = tf.sign(decoder_inputs)\n",
    "\n",
    "    if args.tied_embedding:\n",
    "        vs = tf.variable_scope('encoder_embedding', reuse=True)\n",
    "    else:\n",
    "        vs = tf.variable_scope('decoder_embedding')\n",
    "    with vs:\n",
    "        decoded = embed_seq(decoder_inputs,\n",
    "                            params['target_vocab_size'],\n",
    "                            args.hidden_units,\n",
    "                            zero_pad = True,\n",
    "                            scale = True)\n",
    "\n",
    "    with tf.variable_scope('decoder_position_encoding'):\n",
    "        decoded += pos_enc(decoder_inputs, de_masks, args.hidden_units)\n",
    "\n",
    "    with tf.variable_scope('decoder_dropout'):\n",
    "        decoded = tf.layers.dropout(decoded, args.dropout_rate, training=is_training)\n",
    "\n",
    "    for i in range(args.num_blocks):\n",
    "        with tf.variable_scope('decoder_self_attn_%d'%i):\n",
    "            decoded = multihead_attn(queries = decoded,\n",
    "                                     keys = decoded,\n",
    "                                     q_masks = de_masks,\n",
    "                                     k_masks = de_masks,\n",
    "                                     future_binding = True,\n",
    "                                     is_training = is_training)\n",
    "\n",
    "        with tf.variable_scope('decoder_attn_%d'%i):\n",
    "            decoded = multihead_attn(queries=decoded,\n",
    "                                     keys = encoded,\n",
    "                                     q_masks = de_masks,\n",
    "                                     k_masks = en_masks,\n",
    "                                     future_binding = False,\n",
    "                                     is_training = is_training)\n",
    "\n",
    "        with tf.variable_scope('decoder_feedforward_%d'%i):\n",
    "            decoded = pointwise_feedforward(decoded,\n",
    "                                            activation = params['activation'])\n",
    "\n",
    "    # OUTPUT LAYER    \n",
    "    if args.tied_proj_weight:\n",
    "        b = tf.get_variable('bias', [params['target_vocab_size']], tf.float32)\n",
    "        _scope = 'encoder_embedding' if args.tied_embedding else 'decoder_embedding'\n",
    "        with tf.variable_scope(_scope, reuse=True):\n",
    "            shared_w = tf.get_variable('lookup_table')\n",
    "        decoded = tf.reshape(decoded, [-1, args.hidden_units])\n",
    "        logits = tf.nn.xw_plus_b(decoded, tf.transpose(shared_w), b)\n",
    "        logits = tf.reshape(logits, [tf.shape(sources)[0], -1, params['target_vocab_size']])\n",
    "    else:\n",
    "        with tf.variable_scope('output_layer'):\n",
    "            logits = tf.layers.dense(decoded, params['target_vocab_size'], reuse=reuse)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def _model_fn_train(features, mode, params):\n",
    "    logits = forward_pass(features['source'], features['target'], mode, params)\n",
    "    targets = features['target']\n",
    "    masks = tf.to_float(tf.not_equal(targets, 0))\n",
    "\n",
    "    if args.label_smoothing:\n",
    "        loss_op = label_smoothing_sequence_loss(logits = logits,\n",
    "                                                targets = targets,\n",
    "                                                weights = masks,\n",
    "                                                label_depth = params['target_vocab_size'])\n",
    "    else:\n",
    "        loss_op = tf.contrib.seq2seq.sequence_loss(logits = logits,\n",
    "                                                   targets = targets,\n",
    "                                                   weights = masks)\n",
    "\n",
    "    if args.lr_decay_strategy == 'noam':\n",
    "        step_num = tf.train.get_global_step() + 1   # prevents zero global step\n",
    "        lr = _get_noam_lr(step_num)\n",
    "    elif args.lr_decay_strategy == 'exp':\n",
    "        lr = tf.train.exponential_decay(1e-3, tf.train.get_global_step(), 100000, 0.1)\n",
    "    else:\n",
    "        raise ValueError(\"lr decay strategy must be one of 'noam' and 'exp'\")\n",
    "    log_hook = tf.train.LoggingTensorHook({'lr': lr}, every_n_iter=100)\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op,\n",
    "                                                   global_step = tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode = mode,\n",
    "                                      loss = loss_op,\n",
    "                                      train_op = train_op,\n",
    "                                      training_hooks = [log_hook])\n",
    "\n",
    "\n",
    "def _model_fn_predict(features, mode, params):\n",
    "    def cond(i, x, temp):\n",
    "        return i < args.target_max_len\n",
    "\n",
    "    def body(i, x, temp):\n",
    "        logits = forward_pass(features['source'], x, mode, params)\n",
    "        ids = tf.argmax(logits, -1)[:, i]\n",
    "        ids = tf.expand_dims(ids, -1)\n",
    "\n",
    "        temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "\n",
    "        x = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "        x = tf.reshape(x, [tf.shape(temp)[0], args.target_max_len])\n",
    "        i += 1\n",
    "        return i, x, temp\n",
    "\n",
    "    _, res, _ = tf.while_loop(cond, body, [tf.constant(0), features['target'], features['target']])\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=res)\n",
    "\n",
    "\n",
    "def tf_estimator_model_fn(features, labels, mode, params):\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return _model_fn_train(features, mode, params)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return _model_fn_predict(features, mode, params)\n",
    "\n",
    "\n",
    "def _shift_right(targets, start_symbol):\n",
    "    start_symbols = tf.cast(tf.fill([tf.shape(targets)[0], 1], start_symbol), tf.int64)\n",
    "    return tf.concat([start_symbols, targets[:, :-1]], axis=-1)\n",
    "\n",
    "\n",
    "def _get_position_encoder():\n",
    "    if args.position_encoding == 'non_param':\n",
    "        pos_enc = sinusoidal_position_encoding\n",
    "    elif args.position_encoding == 'param':\n",
    "        pos_enc = learned_position_encoding\n",
    "    else:\n",
    "        raise ValueError(\"position encoding has to be either 'param' or 'non_param'\")\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "def _get_noam_lr(step_num):\n",
    "    return tf.rsqrt(tf.to_float(args.hidden_units)) * tf.minimum(\n",
    "        tf.rsqrt(tf.to_float(step_num)),\n",
    "        tf.to_float(step_num) * tf.convert_to_tensor(args.warmup_steps ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocab Size: 2022\n",
      "Target Vocab Size: 2481\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1196046a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.790139, step = 1\n",
      "INFO:tensorflow:lr = 0.001\n",
      "INFO:tensorflow:global_step/sec: 5.34454\n",
      "INFO:tensorflow:loss = 5.1358037, step = 101 (18.712 sec)\n",
      "INFO:tensorflow:lr = 0.0009977 (18.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.45271\n",
      "INFO:tensorflow:loss = 4.1558905, step = 201 (18.339 sec)\n",
      "INFO:tensorflow:lr = 0.0009954055 (18.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.40337\n",
      "INFO:tensorflow:loss = 4.265932, step = 301 (18.508 sec)\n",
      "INFO:tensorflow:lr = 0.0009931161 (18.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92064\n",
      "INFO:tensorflow:loss = 4.276378, step = 401 (16.890 sec)\n",
      "INFO:tensorflow:lr = 0.000990832 (16.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.80249\n",
      "INFO:tensorflow:loss = 4.147038, step = 501 (17.234 sec)\n",
      "INFO:tensorflow:lr = 0.0009885532 (17.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.07051\n",
      "INFO:tensorflow:loss = 4.257392, step = 601 (16.473 sec)\n",
      "INFO:tensorflow:lr = 0.0009862796 (16.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.48256\n",
      "INFO:tensorflow:loss = 3.984517, step = 701 (15.427 sec)\n",
      "INFO:tensorflow:lr = 0.0009840112 (15.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.43816\n",
      "INFO:tensorflow:loss = 4.1726995, step = 801 (15.531 sec)\n",
      "INFO:tensorflow:lr = 0.000981748 (15.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.37517\n",
      "INFO:tensorflow:loss = 3.8227894, step = 901 (15.686 sec)\n",
      "INFO:tensorflow:lr = 0.00097949 (15.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39021\n",
      "INFO:tensorflow:loss = 3.8545334, step = 1001 (15.649 sec)\n",
      "INFO:tensorflow:lr = 0.0009772372 (15.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39477\n",
      "INFO:tensorflow:loss = 3.727521, step = 1101 (15.638 sec)\n",
      "INFO:tensorflow:lr = 0.00097498967 (15.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.42674\n",
      "INFO:tensorflow:loss = 4.03527, step = 1201 (15.560 sec)\n",
      "INFO:tensorflow:lr = 0.0009727473 (15.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.37087\n",
      "INFO:tensorflow:loss = 3.6047578, step = 1301 (15.696 sec)\n",
      "INFO:tensorflow:lr = 0.00097051 (15.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.61253\n",
      "INFO:tensorflow:loss = 3.7241416, step = 1401 (15.123 sec)\n",
      "INFO:tensorflow:lr = 0.0009682779 (15.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.52338\n",
      "INFO:tensorflow:loss = 3.7886012, step = 1501 (15.329 sec)\n",
      "INFO:tensorflow:lr = 0.0009660509 (15.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.57187\n",
      "INFO:tensorflow:loss = 3.8029654, step = 1601 (15.216 sec)\n",
      "INFO:tensorflow:lr = 0.0009638291 (15.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2207\n",
      "INFO:tensorflow:loss = 3.9436066, step = 1701 (16.075 sec)\n",
      "INFO:tensorflow:lr = 0.0009616123 (16.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.43985\n",
      "INFO:tensorflow:loss = 3.570044, step = 1801 (15.528 sec)\n",
      "INFO:tensorflow:lr = 0.0009594007 (15.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.58928\n",
      "INFO:tensorflow:loss = 3.584323, step = 1901 (15.176 sec)\n",
      "INFO:tensorflow:lr = 0.00095719413 (15.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.49668\n",
      "INFO:tensorflow:loss = 3.6005776, step = 2001 (15.393 sec)\n",
      "INFO:tensorflow:lr = 0.00095499266 (15.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.33268\n",
      "INFO:tensorflow:loss = 3.9795003, step = 2101 (15.791 sec)\n",
      "INFO:tensorflow:lr = 0.0009527962 (15.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46184\n",
      "INFO:tensorflow:loss = 3.7901635, step = 2201 (15.476 sec)\n",
      "INFO:tensorflow:lr = 0.00095060485 (15.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09905\n",
      "INFO:tensorflow:loss = 3.8166916, step = 2301 (16.396 sec)\n",
      "INFO:tensorflow:lr = 0.00094841846 (16.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35533\n",
      "INFO:tensorflow:loss = 3.371935, step = 2401 (15.735 sec)\n",
      "INFO:tensorflow:lr = 0.0009462372 (15.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30065\n",
      "INFO:tensorflow:loss = 3.2786696, step = 2501 (15.871 sec)\n",
      "INFO:tensorflow:lr = 0.0009440609 (15.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.56621\n",
      "INFO:tensorflow:loss = 3.2769418, step = 2601 (15.229 sec)\n",
      "INFO:tensorflow:lr = 0.00094188965 (15.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.48246\n",
      "INFO:tensorflow:loss = 3.3268974, step = 2701 (15.426 sec)\n",
      "INFO:tensorflow:lr = 0.00093972334 (15.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.49304\n",
      "INFO:tensorflow:loss = 3.458511, step = 2801 (15.401 sec)\n",
      "INFO:tensorflow:lr = 0.000937562 (15.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28776\n",
      "INFO:tensorflow:loss = 3.440421, step = 2901 (15.904 sec)\n",
      "INFO:tensorflow:lr = 0.0009354057 (15.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.42651\n",
      "INFO:tensorflow:loss = 3.442154, step = 3001 (15.561 sec)\n",
      "INFO:tensorflow:lr = 0.00093325437 (15.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28207\n",
      "INFO:tensorflow:loss = 3.6312141, step = 3101 (15.918 sec)\n",
      "INFO:tensorflow:lr = 0.0009311079 (15.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.5207\n",
      "INFO:tensorflow:loss = 3.4699922, step = 3201 (15.336 sec)\n",
      "INFO:tensorflow:lr = 0.00092896644 (15.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54055\n",
      "INFO:tensorflow:loss = 3.3394067, step = 3301 (15.289 sec)\n",
      "INFO:tensorflow:lr = 0.0009268299 (15.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.62246\n",
      "INFO:tensorflow:loss = 3.3993046, step = 3401 (15.100 sec)\n",
      "INFO:tensorflow:lr = 0.0009246982 (15.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.59374\n",
      "INFO:tensorflow:loss = 3.3347147, step = 3501 (15.166 sec)\n",
      "INFO:tensorflow:lr = 0.00092257146 (15.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.29232\n",
      "INFO:tensorflow:loss = 3.3047163, step = 3601 (15.892 sec)\n",
      "INFO:tensorflow:lr = 0.0009204496 (15.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46623\n",
      "INFO:tensorflow:loss = 3.0544121, step = 3701 (15.465 sec)\n",
      "INFO:tensorflow:lr = 0.0009183326 (15.465 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3739 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.77089\n",
      "INFO:tensorflow:loss = 3.23316, step = 3801 (20.961 sec)\n",
      "INFO:tensorflow:lr = 0.00091622054 (20.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93012\n",
      "INFO:tensorflow:loss = 3.2531655, step = 3901 (16.863 sec)\n",
      "INFO:tensorflow:lr = 0.00091411325 (16.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.67437\n",
      "INFO:tensorflow:loss = 3.1231744, step = 4001 (17.624 sec)\n",
      "INFO:tensorflow:lr = 0.0009120109 (17.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.63245\n",
      "INFO:tensorflow:loss = 3.1345382, step = 4101 (17.753 sec)\n",
      "INFO:tensorflow:lr = 0.00090991333 (17.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45198\n",
      "INFO:tensorflow:loss = 2.9397807, step = 4201 (15.499 sec)\n",
      "INFO:tensorflow:lr = 0.0009078206 (15.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.33535\n",
      "INFO:tensorflow:loss = 3.537605, step = 4301 (15.785 sec)\n",
      "INFO:tensorflow:lr = 0.0009057327 (15.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.91302\n",
      "INFO:tensorflow:loss = 3.0130115, step = 4401 (16.912 sec)\n",
      "INFO:tensorflow:lr = 0.0009036495 (16.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.03531\n",
      "INFO:tensorflow:loss = 3.1043897, step = 4501 (16.568 sec)\n",
      "INFO:tensorflow:lr = 0.0009015712 (16.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09212\n",
      "INFO:tensorflow:loss = 3.144913, step = 4601 (16.415 sec)\n",
      "INFO:tensorflow:lr = 0.0008994976 (16.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 2.7595823, step = 4701 (16.802 sec)\n",
      "INFO:tensorflow:lr = 0.00089742884 (16.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.75022\n",
      "INFO:tensorflow:loss = 3.0420403, step = 4801 (17.391 sec)\n",
      "INFO:tensorflow:lr = 0.0008953648 (17.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9127\n",
      "INFO:tensorflow:loss = 2.698546, step = 4901 (16.913 sec)\n",
      "INFO:tensorflow:lr = 0.0008933055 (16.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.10724\n",
      "INFO:tensorflow:loss = 2.6791887, step = 5001 (16.374 sec)\n",
      "INFO:tensorflow:lr = 0.000891251 (16.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44342\n",
      "INFO:tensorflow:loss = 2.7579823, step = 5101 (15.520 sec)\n",
      "INFO:tensorflow:lr = 0.0008892011 (15.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01166\n",
      "INFO:tensorflow:loss = 2.817311, step = 5201 (16.634 sec)\n",
      "INFO:tensorflow:lr = 0.00088715606 (16.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.77916\n",
      "INFO:tensorflow:loss = 2.7982013, step = 5301 (17.303 sec)\n",
      "INFO:tensorflow:lr = 0.00088511565 (17.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.79776\n",
      "INFO:tensorflow:loss = 2.917202, step = 5401 (17.249 sec)\n",
      "INFO:tensorflow:lr = 0.00088307995 (17.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09631\n",
      "INFO:tensorflow:loss = 2.9771326, step = 5501 (16.402 sec)\n",
      "INFO:tensorflow:lr = 0.0008810489 (16.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.90885\n",
      "INFO:tensorflow:loss = 2.952243, step = 5601 (16.924 sec)\n",
      "INFO:tensorflow:lr = 0.0008790226 (16.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93715\n",
      "INFO:tensorflow:loss = 2.3678827, step = 5701 (16.843 sec)\n",
      "INFO:tensorflow:lr = 0.00087700086 (16.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.81369\n",
      "INFO:tensorflow:loss = 3.39819, step = 5801 (17.200 sec)\n",
      "INFO:tensorflow:lr = 0.00087498385 (17.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50547\n",
      "INFO:tensorflow:loss = 2.9422264, step = 5901 (15.372 sec)\n",
      "INFO:tensorflow:lr = 0.0008729714 (15.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19834\n",
      "INFO:tensorflow:loss = 2.5407176, step = 6001 (16.134 sec)\n",
      "INFO:tensorflow:lr = 0.0008709636 (16.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.00685\n",
      "INFO:tensorflow:loss = 2.7291286, step = 6101 (16.648 sec)\n",
      "INFO:tensorflow:lr = 0.00086896046 (16.648 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01734\n",
      "INFO:tensorflow:loss = 2.4127803, step = 6201 (16.619 sec)\n",
      "INFO:tensorflow:lr = 0.00086696196 (16.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.169\n",
      "INFO:tensorflow:loss = 3.0972435, step = 6301 (16.210 sec)\n",
      "INFO:tensorflow:lr = 0.000864968 (16.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.08212\n",
      "INFO:tensorflow:loss = 2.8682342, step = 6401 (16.442 sec)\n",
      "INFO:tensorflow:lr = 0.00086297863 (16.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28457\n",
      "INFO:tensorflow:loss = 2.9093602, step = 6501 (15.912 sec)\n",
      "INFO:tensorflow:lr = 0.0008609938 (15.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19376\n",
      "INFO:tensorflow:loss = 2.7623396, step = 6601 (16.145 sec)\n",
      "INFO:tensorflow:lr = 0.0008590135 (16.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.31612\n",
      "INFO:tensorflow:loss = 3.1531844, step = 6701 (15.833 sec)\n",
      "INFO:tensorflow:lr = 0.0008570379 (15.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09045\n",
      "INFO:tensorflow:loss = 3.3342836, step = 6801 (16.419 sec)\n",
      "INFO:tensorflow:lr = 0.00085506676 (16.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.41983\n",
      "INFO:tensorflow:loss = 3.1780925, step = 6901 (15.577 sec)\n",
      "INFO:tensorflow:lr = 0.00085310015 (15.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39467\n",
      "INFO:tensorflow:loss = 3.2201087, step = 7001 (15.638 sec)\n",
      "INFO:tensorflow:lr = 0.0008511381 (15.638 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7092 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.2470026.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt-7092\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "你是谁 -> 我是小通\n",
      "你喜欢我吗 -> 我是小通\n",
      "给我唱一首歌 -> 我是你的小通\n",
      "我帅吗 -> 我是你的小通\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt-7092\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 7093 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2608647, step = 7093\n",
      "INFO:tensorflow:lr = 0.00084933697\n",
      "INFO:tensorflow:global_step/sec: 5.68964\n",
      "INFO:tensorflow:loss = 3.3583546, step = 7193 (17.577 sec)\n",
      "INFO:tensorflow:lr = 0.0008473835 (17.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.21078\n",
      "INFO:tensorflow:loss = 3.33242, step = 7293 (16.101 sec)\n",
      "INFO:tensorflow:lr = 0.00084543467 (16.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.29809\n",
      "INFO:tensorflow:loss = 3.0829463, step = 7393 (15.878 sec)\n",
      "INFO:tensorflow:lr = 0.0008434902 (15.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46005\n",
      "INFO:tensorflow:loss = 3.386406, step = 7493 (15.480 sec)\n",
      "INFO:tensorflow:lr = 0.00084155024 (15.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.49455\n",
      "INFO:tensorflow:loss = 2.850423, step = 7593 (15.397 sec)\n",
      "INFO:tensorflow:lr = 0.00083961466 (15.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39827\n",
      "INFO:tensorflow:loss = 3.3763106, step = 7693 (15.629 sec)\n",
      "INFO:tensorflow:lr = 0.0008376836 (15.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39848\n",
      "INFO:tensorflow:loss = 3.2479782, step = 7793 (15.629 sec)\n",
      "INFO:tensorflow:lr = 0.000835757 (15.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39861\n",
      "INFO:tensorflow:loss = 2.8985934, step = 7893 (15.629 sec)\n",
      "INFO:tensorflow:lr = 0.0008338348 (15.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.0392\n",
      "INFO:tensorflow:loss = 3.6845367, step = 7993 (16.558 sec)\n",
      "INFO:tensorflow:lr = 0.00083191704 (16.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.23758\n",
      "INFO:tensorflow:loss = 2.804047, step = 8093 (16.032 sec)\n",
      "INFO:tensorflow:lr = 0.0008300037 (16.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17945\n",
      "INFO:tensorflow:loss = 2.9537735, step = 8193 (16.183 sec)\n",
      "INFO:tensorflow:lr = 0.0008280948 (16.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.15591\n",
      "INFO:tensorflow:loss = 3.0522711, step = 8293 (16.245 sec)\n",
      "INFO:tensorflow:lr = 0.00082619017 (16.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.113\n",
      "INFO:tensorflow:loss = 2.8053799, step = 8393 (16.359 sec)\n",
      "INFO:tensorflow:lr = 0.00082429004 (16.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98508\n",
      "INFO:tensorflow:loss = 2.972553, step = 8493 (16.708 sec)\n",
      "INFO:tensorflow:lr = 0.00082239415 (16.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.33731\n",
      "INFO:tensorflow:loss = 2.8280582, step = 8593 (15.780 sec)\n",
      "INFO:tensorflow:lr = 0.00082050276 (15.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3048\n",
      "INFO:tensorflow:loss = 2.9988227, step = 8693 (15.861 sec)\n",
      "INFO:tensorflow:lr = 0.0008186156 (15.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44438\n",
      "INFO:tensorflow:loss = 3.471268, step = 8793 (15.517 sec)\n",
      "INFO:tensorflow:lr = 0.0008167329 (15.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.25059\n",
      "INFO:tensorflow:loss = 2.9821384, step = 8893 (15.999 sec)\n",
      "INFO:tensorflow:lr = 0.0008148544 (15.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.29967\n",
      "INFO:tensorflow:loss = 2.9231076, step = 8993 (15.874 sec)\n",
      "INFO:tensorflow:lr = 0.0008129803 (15.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28937\n",
      "INFO:tensorflow:loss = 3.0756195, step = 9093 (15.901 sec)\n",
      "INFO:tensorflow:lr = 0.0008111105 (15.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93826\n",
      "INFO:tensorflow:loss = 3.0184753, step = 9193 (16.839 sec)\n",
      "INFO:tensorflow:lr = 0.00080924504 (16.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01653\n",
      "INFO:tensorflow:loss = 2.7636492, step = 9293 (16.621 sec)\n",
      "INFO:tensorflow:lr = 0.0008073838 (16.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09341\n",
      "INFO:tensorflow:loss = 2.917549, step = 9393 (16.411 sec)\n",
      "INFO:tensorflow:lr = 0.00080552686 (16.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.80678\n",
      "INFO:tensorflow:loss = 2.9949796, step = 9493 (17.221 sec)\n",
      "INFO:tensorflow:lr = 0.0008036742 (17.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.63096\n",
      "INFO:tensorflow:loss = 3.0658324, step = 9593 (17.759 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:lr = 0.0008018258 (17.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94603\n",
      "INFO:tensorflow:loss = 2.512178, step = 9693 (16.818 sec)\n",
      "INFO:tensorflow:lr = 0.00079998164 (16.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12279\n",
      "INFO:tensorflow:loss = 2.7818282, step = 9793 (16.332 sec)\n",
      "INFO:tensorflow:lr = 0.00079814176 (16.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35533\n",
      "INFO:tensorflow:loss = 2.8232658, step = 9893 (15.735 sec)\n",
      "INFO:tensorflow:lr = 0.00079630606 (15.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2289\n",
      "INFO:tensorflow:loss = 3.5558274, step = 9993 (16.054 sec)\n",
      "INFO:tensorflow:lr = 0.0007944746 (16.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.42233\n",
      "INFO:tensorflow:loss = 2.648949, step = 10093 (15.571 sec)\n",
      "INFO:tensorflow:lr = 0.00079264736 (15.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39169\n",
      "INFO:tensorflow:loss = 2.793739, step = 10193 (15.645 sec)\n",
      "INFO:tensorflow:lr = 0.00079082436 (15.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.40585\n",
      "INFO:tensorflow:loss = 2.9148273, step = 10293 (15.611 sec)\n",
      "INFO:tensorflow:lr = 0.0007890055 (15.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.00333\n",
      "INFO:tensorflow:loss = 2.4817865, step = 10393 (16.658 sec)\n",
      "INFO:tensorflow:lr = 0.00078719086 (16.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.07723\n",
      "INFO:tensorflow:loss = 2.6302285, step = 10493 (16.455 sec)\n",
      "INFO:tensorflow:lr = 0.00078538037 (16.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.04032\n",
      "INFO:tensorflow:loss = 2.6961613, step = 10593 (16.555 sec)\n",
      "INFO:tensorflow:lr = 0.000783574 (16.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.79914\n",
      "INFO:tensorflow:loss = 2.988709, step = 10693 (17.244 sec)\n",
      "INFO:tensorflow:lr = 0.00078177184 (17.244 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10756 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.61874\n",
      "INFO:tensorflow:loss = 2.7945306, step = 10793 (21.651 sec)\n",
      "INFO:tensorflow:lr = 0.00077997387 (21.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2292\n",
      "INFO:tensorflow:loss = 2.964288, step = 10893 (16.053 sec)\n",
      "INFO:tensorflow:lr = 0.0007781799 (16.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.06358\n",
      "INFO:tensorflow:loss = 3.0429878, step = 10993 (16.492 sec)\n",
      "INFO:tensorflow:lr = 0.0007763902 (16.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35683\n",
      "INFO:tensorflow:loss = 2.5641956, step = 11093 (15.731 sec)\n",
      "INFO:tensorflow:lr = 0.00077460456 (15.730 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36041\n",
      "INFO:tensorflow:loss = 2.351427, step = 11193 (15.722 sec)\n",
      "INFO:tensorflow:lr = 0.00077282294 (15.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.20344\n",
      "INFO:tensorflow:loss = 2.9592535, step = 11293 (16.120 sec)\n",
      "INFO:tensorflow:lr = 0.00077104557 (16.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.40234\n",
      "INFO:tensorflow:loss = 3.1931648, step = 11393 (15.620 sec)\n",
      "INFO:tensorflow:lr = 0.0007692722 (15.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3836\n",
      "INFO:tensorflow:loss = 3.2871668, step = 11493 (15.665 sec)\n",
      "INFO:tensorflow:lr = 0.0007675029 (15.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36531\n",
      "INFO:tensorflow:loss = 2.425646, step = 11593 (15.710 sec)\n",
      "INFO:tensorflow:lr = 0.00076573767 (15.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.38849\n",
      "INFO:tensorflow:loss = 2.53078, step = 11693 (15.653 sec)\n",
      "INFO:tensorflow:lr = 0.00076397654 (15.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36857\n",
      "INFO:tensorflow:loss = 2.3861146, step = 11793 (15.703 sec)\n",
      "INFO:tensorflow:lr = 0.0007622195 (15.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.37037\n",
      "INFO:tensorflow:loss = 2.480415, step = 11893 (15.697 sec)\n",
      "INFO:tensorflow:lr = 0.0007604664 (15.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3213\n",
      "INFO:tensorflow:loss = 2.2141666, step = 11993 (15.820 sec)\n",
      "INFO:tensorflow:lr = 0.00075871736 (15.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.03997\n",
      "INFO:tensorflow:loss = 2.6142294, step = 12093 (16.556 sec)\n",
      "INFO:tensorflow:lr = 0.00075697235 (16.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95453\n",
      "INFO:tensorflow:loss = 2.7326887, step = 12193 (16.794 sec)\n",
      "INFO:tensorflow:lr = 0.00075523136 (16.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35269\n",
      "INFO:tensorflow:loss = 2.803204, step = 12293 (15.741 sec)\n",
      "INFO:tensorflow:lr = 0.00075349445 (15.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30936\n",
      "INFO:tensorflow:loss = 3.104464, step = 12393 (15.849 sec)\n",
      "INFO:tensorflow:lr = 0.0007517614 (15.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17706\n",
      "INFO:tensorflow:loss = 3.2493513, step = 12493 (16.189 sec)\n",
      "INFO:tensorflow:lr = 0.0007500324 (16.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27235\n",
      "INFO:tensorflow:loss = 3.3200622, step = 12593 (15.943 sec)\n",
      "INFO:tensorflow:lr = 0.0007483074 (15.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54393\n",
      "INFO:tensorflow:loss = 3.2838302, step = 12693 (15.281 sec)\n",
      "INFO:tensorflow:lr = 0.0007465863 (15.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30941\n",
      "INFO:tensorflow:loss = 3.3016891, step = 12793 (15.849 sec)\n",
      "INFO:tensorflow:lr = 0.00074486923 (15.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.33768\n",
      "INFO:tensorflow:loss = 3.42035, step = 12893 (15.779 sec)\n",
      "INFO:tensorflow:lr = 0.00074315607 (15.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.23579\n",
      "INFO:tensorflow:loss = 3.4431283, step = 12993 (16.036 sec)\n",
      "INFO:tensorflow:lr = 0.0007414469 (16.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93518\n",
      "INFO:tensorflow:loss = 3.3174765, step = 13093 (16.849 sec)\n",
      "INFO:tensorflow:lr = 0.0007397416 (16.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.37866\n",
      "INFO:tensorflow:loss = 3.4159026, step = 13193 (15.677 sec)\n",
      "INFO:tensorflow:lr = 0.00073804025 (15.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50301\n",
      "INFO:tensorflow:loss = 3.552766, step = 13293 (15.377 sec)\n",
      "INFO:tensorflow:lr = 0.00073634274 (15.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.0549\n",
      "INFO:tensorflow:loss = 3.1002545, step = 13393 (16.516 sec)\n",
      "INFO:tensorflow:lr = 0.0007346492 (16.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4192\n",
      "INFO:tensorflow:loss = 3.1900213, step = 13493 (15.578 sec)\n",
      "INFO:tensorflow:lr = 0.00073295954 (15.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3192\n",
      "INFO:tensorflow:loss = 3.0194113, step = 13593 (15.825 sec)\n",
      "INFO:tensorflow:lr = 0.0007312738 (15.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4057\n",
      "INFO:tensorflow:loss = 3.0196173, step = 13693 (15.611 sec)\n",
      "INFO:tensorflow:lr = 0.00072959193 (15.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35562\n",
      "INFO:tensorflow:loss = 3.284308, step = 13793 (15.734 sec)\n",
      "INFO:tensorflow:lr = 0.0007279139 (15.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.00034\n",
      "INFO:tensorflow:loss = 2.884755, step = 13893 (16.666 sec)\n",
      "INFO:tensorflow:lr = 0.00072623976 (16.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27721\n",
      "INFO:tensorflow:loss = 3.1739674, step = 13993 (15.931 sec)\n",
      "INFO:tensorflow:lr = 0.0007245695 (15.931 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46578\n",
      "INFO:tensorflow:loss = 3.042697, step = 14093 (15.466 sec)\n",
      "INFO:tensorflow:lr = 0.000722903 (15.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14184 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.0361738.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp4hzsnejb/model.ckpt-14184\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "你是谁 -> 我是小通\n",
      "你喜欢我吗 -> 我喜欢你\n",
      "给我唱一首歌 -> 我是小通\n",
      "我帅吗 -> 你是我的优乐美\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(test_words, tf_estimator, dl):\n",
    "    test_indices = []\n",
    "    for test_word in test_words:\n",
    "        test_idx = [dl.source_word2idx[c] for c in test_word] + \\\n",
    "                   [dl.source_word2idx['<pad>']] * (args.source_max_len - len(test_word))\n",
    "        test_indices.append(test_idx)\n",
    "    test_indices = np.atleast_2d(test_indices)\n",
    "    \n",
    "    zeros = np.zeros([len(test_words), args.target_max_len], np.int64)\n",
    "\n",
    "    pred_ids = tf_estimator.predict(tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'source':test_indices, 'target':zeros}, batch_size=len(test_words), shuffle=False))\n",
    "    pred_ids = list(pred_ids)\n",
    "    \n",
    "    target_idx2word = {i: w for w, i in dl.target_word2idx.items()}\n",
    "    for i, test_word in enumerate(test_words):\n",
    "        ans = ''.join([target_idx2word[id] for id in pred_ids[i]])\n",
    "        print(test_word, '->', ans.replace('<end>', ''))\n",
    "\n",
    "\n",
    "def prepare_params(dl):\n",
    "    if args.activation == 'relu':\n",
    "        activation = tf.nn.relu\n",
    "    elif args.activation == 'elu':\n",
    "        activation = tf.nn.elu\n",
    "    elif args.activation == 'lrelu':\n",
    "        activation = tf.nn.leaky_relu\n",
    "    else:\n",
    "        raise ValueError(\"acitivation fn has to be 'relu' or 'elu' or 'lrelu'\")\n",
    "    params = {\n",
    "        'source_vocab_size': len(dl.source_word2idx),\n",
    "        'target_vocab_size': len(dl.target_word2idx),\n",
    "        'start_symbol': dl.target_word2idx['<start>'],\n",
    "        'activation': activation}\n",
    "    return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    dl = DataLoader(\n",
    "        source_path='../temp/dialog_source.txt',\n",
    "        target_path='../temp/dialog_target.txt')\n",
    "    sources, targets = dl.load()\n",
    "    print('Source Vocab Size:', len(dl.source_word2idx))\n",
    "    print('Target Vocab Size:', len(dl.target_word2idx))\n",
    "    \n",
    "    tf_estimator = tf.estimator.Estimator(\n",
    "        tf_estimator_model_fn, params=prepare_params(dl))\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        tf_estimator.train(tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {'source':sources, 'target':targets},\n",
    "            batch_size = args.batch_size,\n",
    "            shuffle = True))\n",
    "        greedy_decode(['你是谁', '你喜欢我吗', '给我唱一首歌', '我帅吗'], tf_estimator, dl)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
